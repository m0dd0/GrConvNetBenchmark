{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "from grconvnet.dataloading.datasets import YCBSimulationData\n",
    "from grconvnet.utils.processing import End2EndProcessor\n",
    "from grconvnet.postprocessing import Img2WorldCoordConverter, Decropper\n",
    "from grconvnet.utils.config import module_from_config\n",
    "from grconvnet.utils import visualization as vis\n",
    "from grconvnet.utils.export import Exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(\"/home/moritz/Documents/ycb_sim_data_1\")\n",
    "config_path = Path.cwd().parent / \"grconvnet\" / \"configs\" / \"ycb_inference_standard.yaml\"\n",
    "export_path = Path.cwd().parent / \"grconvnet\" / \"results\" / \"ycb_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'001_master_chef_can'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = YCBSimulationData(dataset_path)\n",
    "sample = dataset[0]\n",
    "sample.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_intrinsics = sample.cam_intrinsics\n",
    "cam_pos = sample.cam_pos\n",
    "cam_rot = sample.cam_rot\n",
    "image_size = sample.rgb.shape[1:]\n",
    "\n",
    "dataset_config = {\n",
    "    \"path\": str(dataset_path),\n",
    "    \"cam_intrinsics\": cam_intrinsics.tolist(),\n",
    "    \"cam_pos\": cam_pos.tolist(),\n",
    "    \"cam_rot\": cam_rot.tolist(),\n",
    "    \"image_size\": list(image_size), \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2e_processor = module_from_config(config)\n",
    "\n",
    "e2e_processor.img2world_converter.coord_converter = Img2WorldCoordConverter(\n",
    "    cam_intrinsics, cam_rot, cam_pos\n",
    ")\n",
    "e2e_processor.img2world_converter.decropper = Decropper(\n",
    "    resized_in_preprocess=config[\"preprocessor\"][\"resize\"], original_img_size=image_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exporter = Exporter(export_dir=export_path)\n",
    "\n",
    "export_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(export_path / \"inference_config.yaml\", \"w\") as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "with open(export_path / \"dataset_config.yaml\", \"w\") as f:\n",
    "    yaml.dump(dataset_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample 001_master_chef_can...\n",
      "Processing sample 002_cracker_box...\n",
      "Processing sample 003_sugar_box...\n",
      "Processing sample 004_tomato_soup_can...\n",
      "Processing sample 005_mustard_bottle...\n",
      "Processing sample 006_tuna_fish_can...\n",
      "Processing sample 007_pudding_box...\n",
      "Processing sample 008_gelatin_box...\n",
      "Processing sample 009_potted_meat_can...\n",
      "Processing sample 010_banana...\n",
      "Processing sample 011_strawberry...\n",
      "Processing sample 012_apple...\n",
      "Processing sample 013_lemon...\n",
      "Processing sample 014_peach...\n",
      "Processing sample 015_pear...\n",
      "Processing sample 016_orange...\n",
      "Processing sample 017_plum...\n",
      "Processing sample 018_pitcher_base...\n",
      "Processing sample 019_bleach_cleanser...\n",
      "Processing sample 021_bowl...\n",
      "Processing sample 022_mug...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9653/334740947.py:12: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig=plt.figure(figsize=(20, 20)),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample 023_sponge...\n",
      "Processing sample 025_plate...\n",
      "Processing sample 026_fork...\n",
      "Processing sample 027_spoon...\n",
      "Processing sample 028_knife...\n",
      "Processing sample 029_spatula...\n",
      "Processing sample 030_power_drill...\n",
      "Processing sample 031_wood_block...\n",
      "Processing sample 032_scissors...\n",
      "Processing sample 034_large_marker...\n",
      "Processing sample 036_adjustable_wrench...\n",
      "Processing sample 037_phillips_screwdriver...\n",
      "Processing sample 038_flat_screwdriver...\n",
      "Processing sample 039_hammer...\n",
      "Processing sample 041_medium_clamp...\n",
      "Processing sample 042_large_clamp...\n",
      "Processing sample 043_extra_large_clamp...\n",
      "Processing sample 044_mini_soccer_ball...\n",
      "Processing sample 045_softball...\n",
      "Processing sample 046_baseball...\n",
      "Processing sample 047_tennis_ball...\n",
      "Processing sample 048_racquetball...\n",
      "Processing sample 049_golf_ball...\n",
      "Processing sample 050_chain...\n",
      "Processing sample 051_foam_brick...\n",
      "Processing sample 052_dice...\n",
      "Processing sample 053_a_marbles...\n",
      "Processing sample 054_b_marbles...\n",
      "Processing sample 055_a_cups...\n",
      "Processing sample 056_b_cups...\n",
      "Processing sample 057_c_cups...\n",
      "Processing sample 058_d_cups...\n",
      "Processing sample 059_e_cups...\n",
      "Processing sample 060_f_cups...\n",
      "Processing sample 061_g_cups...\n"
     ]
    }
   ],
   "source": [
    "for sample in dataset:\n",
    "    print(f\"Processing sample {sample.name}...\")\n",
    "\n",
    "    assert np.allclose(sample.cam_intrinsics, cam_intrinsics)\n",
    "    assert np.allclose(sample.cam_pos, cam_pos)\n",
    "    assert np.allclose(sample.cam_rot, cam_rot)\n",
    "    assert np.allclose(sample.rgb.shape[1:], image_size)\n",
    "\n",
    "    process_data = e2e_processor(sample)\n",
    "\n",
    "    fig = vis.overview_fig(\n",
    "        fig=plt.figure(figsize=(20, 20)),\n",
    "        original_rgb=vis.make_tensor_displayable(process_data[\"sample\"].rgb, True, True),\n",
    "        preprocessed_rgb=vis.make_tensor_displayable(\n",
    "            process_data[\"preprocessor\"][\"rgb_masked\"], True, True\n",
    "        ),\n",
    "        q_img=vis.make_tensor_displayable(\n",
    "            process_data[\"postprocessor\"][\"q_img\"], False, False\n",
    "        ),\n",
    "        angle_img=vis.make_tensor_displayable(\n",
    "            process_data[\"postprocessor\"][\"angle_img\"], False, False\n",
    "        ),\n",
    "        width_img=vis.make_tensor_displayable(\n",
    "            process_data[\"postprocessor\"][\"width_img\"], False, False\n",
    "        ),\n",
    "        image_grasps=process_data[\"grasps_img\"],\n",
    "        world_grasps=process_data[\"grasps_world\"],\n",
    "        cam_intrinsics=cam_intrinsics,\n",
    "        cam_rot=cam_rot,\n",
    "        cam_pos=cam_pos,\n",
    "    )\n",
    "\n",
    "    export_data = {\n",
    "        \"rgb_cropped\": process_data[\"preprocessor\"][\"rgb_cropped\"],\n",
    "        \"depth_cropped\": process_data[\"preprocessor\"][\"depth_cropped\"],\n",
    "        \"rgb_masked\": process_data[\"preprocessor\"][\"rgb_masked\"],\n",
    "        \"q_img\": process_data[\"postprocessor\"][\"q_img\"],\n",
    "        \"angle_img\": process_data[\"postprocessor\"][\"angle_img\"],\n",
    "        \"width_img\": process_data[\"postprocessor\"][\"width_img\"],\n",
    "        \"grasps_img\": process_data[\"grasps_img\"],\n",
    "        \"grasps_world\": process_data[\"grasps_world\"],\n",
    "        \"model_input\": process_data[\"model_input\"],\n",
    "        \"overview\": fig,\n",
    "    }\n",
    "\n",
    "    export_path = exporter(export_data, f\"{process_data['sample'].name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('alr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ef7e1541b8e7b6b5672ac838d3c045be09c9245709d40ce12336bbbdd1b51144"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
