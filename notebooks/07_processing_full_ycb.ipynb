{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "from grconvnet.dataloading.datasets import YCBSimulationData\n",
    "from grconvnet.utils.processing import End2EndProcessor\n",
    "from grconvnet.postprocessing import Img2WorldCoordConverter, Decropper\n",
    "from grconvnet.utils.config import module_from_config\n",
    "from grconvnet.utils import visualization as vis\n",
    "from grconvnet.utils.export import Exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(\"/home/moritz/Documents/ycb_sim_data_1\")\n",
    "config_path = Path.cwd().parent / \"grconvnet\" / \"configs\" / \"ycb_inference_standard.yaml\"\n",
    "export_path = Path.cwd().parent / \"grconvnet\" / \"results\" / \"ycb_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'001_master_chef_can'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = YCBSimulationData(dataset_path)\n",
    "sample = dataset[0]\n",
    "sample.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_intrinsics = sample.cam_intrinsics\n",
    "cam_pos = sample.cam_pos\n",
    "cam_rot = sample.cam_rot\n",
    "image_size = sample.rgb.shape[1:]\n",
    "\n",
    "dataset_config = {\n",
    "    \"path\": str(dataset_path),\n",
    "    \"cam_intrinsics\": cam_intrinsics.tolist(),\n",
    "    \"cam_pos\": cam_pos.tolist(),\n",
    "    \"cam_rot\": cam_rot.tolist(),\n",
    "    \"image_size\": list(image_size), \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2e_processor = module_from_config(config)\n",
    "\n",
    "e2e_processor.img2world_converter.coord_converter = Img2WorldCoordConverter(\n",
    "    cam_intrinsics, cam_rot, cam_pos\n",
    ")\n",
    "e2e_processor.img2world_converter.decropper = Decropper(\n",
    "    resized_in_preprocess=config[\"preprocessor\"][\"resize\"], original_img_size=image_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exporter = Exporter(export_dir=export_path)\n",
    "\n",
    "export_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(export_path / \"inference_config.yaml\", \"w\") as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "with open(export_path / \"dataset_config.yaml\", \"w\") as f:\n",
    "    yaml.dump(dataset_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample 001_master_chef_can...\n",
      "Processing sample 002_cracker_box...\n",
      "Processing sample 003_sugar_box...\n",
      "Processing sample 004_tomato_soup_can...\n",
      "Processing sample 005_mustard_bottle...\n",
      "Processing sample 006_tuna_fish_can...\n",
      "Processing sample 007_pudding_box...\n",
      "Processing sample 008_gelatin_box...\n",
      "Processing sample 009_potted_meat_can...\n",
      "Processing sample 010_banana...\n",
      "Processing sample 011_strawberry...\n",
      "Processing sample 012_apple...\n",
      "Processing sample 013_lemon...\n",
      "Processing sample 014_peach...\n",
      "Processing sample 015_pear...\n",
      "Processing sample 016_orange...\n",
      "Processing sample 017_plum...\n",
      "Processing sample 018_pitcher_base...\n",
      "Processing sample 019_bleach_cleanser...\n",
      "Processing sample 021_bowl...\n",
      "Processing sample 022_mug...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4479/334740947.py:12: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig=plt.figure(figsize=(20, 20)),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample 023_sponge...\n",
      "Processing sample 025_plate...\n",
      "Processing sample 026_fork...\n",
      "Processing sample 027_spoon...\n",
      "Processing sample 028_knife...\n",
      "Processing sample 029_spatula...\n",
      "Processing sample 030_power_drill...\n",
      "Processing sample 031_wood_block...\n",
      "Processing sample 032_scissors...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39mallclose(sample\u001b[39m.\u001b[39mcam_rot, cam_rot)\n\u001b[1;32m      7\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39mallclose(sample\u001b[39m.\u001b[39mrgb\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m:], image_size)\n\u001b[0;32m----> 9\u001b[0m process_data \u001b[39m=\u001b[39m e2e_processor(sample)\n\u001b[1;32m     11\u001b[0m fig \u001b[39m=\u001b[39m vis\u001b[39m.\u001b[39moverview_fig(\n\u001b[1;32m     12\u001b[0m     fig\u001b[39m=\u001b[39mplt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m20\u001b[39m, \u001b[39m20\u001b[39m)),\n\u001b[1;32m     13\u001b[0m     original_rgb\u001b[39m=\u001b[39mvis\u001b[39m.\u001b[39mmake_tensor_displayable(process_data[\u001b[39m\"\u001b[39m\u001b[39msample\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mrgb, \u001b[39mTrue\u001b[39;00m, \u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     cam_pos\u001b[39m=\u001b[39mcam_pos,\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m export_data \u001b[39m=\u001b[39m {\n\u001b[1;32m     34\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrgb_cropped\u001b[39m\u001b[39m\"\u001b[39m: process_data[\u001b[39m\"\u001b[39m\u001b[39mpreprocessor\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mrgb_cropped\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     35\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdepth_cropped\u001b[39m\u001b[39m\"\u001b[39m: process_data[\u001b[39m\"\u001b[39m\u001b[39mpreprocessor\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mdepth_cropped\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moverview\u001b[39m\u001b[39m\"\u001b[39m: fig,\n\u001b[1;32m     44\u001b[0m }\n",
      "File \u001b[0;32m~/Documents/GrConvNetBenchmark/grconvnet/utils/processing.py:43\u001b[0m, in \u001b[0;36mEnd2EndProcessor.__call__\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m     40\u001b[0m input_tensor \u001b[39m=\u001b[39m input_tensor\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 43\u001b[0m     prediction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(input_tensor)\n\u001b[1;32m     45\u001b[0m grasps_img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocessor(prediction)\n\u001b[1;32m     47\u001b[0m \u001b[39m# TODO save all intermediate results of the img2world converter\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/alr/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GrConvNetBenchmark/grconvnet/models/grconvnet.py:126\u001b[0m, in \u001b[0;36mGenerativeResnet.forward\u001b[0;34m(self, x_in)\u001b[0m\n\u001b[1;32m    124\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mres1(x)\n\u001b[1;32m    125\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mres2(x)\n\u001b[0;32m--> 126\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mres3(x)\n\u001b[1;32m    127\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mres4(x)\n\u001b[1;32m    128\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mres5(x)\n",
      "File \u001b[0;32m~/mambaforge/envs/alr/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GrConvNetBenchmark/grconvnet/models/custom_modules.py:22\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[0;34m(self, x_in)\u001b[0m\n\u001b[1;32m     20\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x_in))\n\u001b[1;32m     21\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(x)\n\u001b[0;32m---> 22\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(x))\n\u001b[1;32m     23\u001b[0m \u001b[39mreturn\u001b[39;00m x \u001b[39m+\u001b[39m x_in\n",
      "File \u001b[0;32m~/mambaforge/envs/alr/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/alr/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/mambaforge/envs/alr/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for sample in dataset:\n",
    "    print(f\"Processing sample {sample.name}...\")\n",
    "\n",
    "    assert np.allclose(sample.cam_intrinsics, cam_intrinsics)\n",
    "    assert np.allclose(sample.cam_pos, cam_pos)\n",
    "    assert np.allclose(sample.cam_rot, cam_rot)\n",
    "    assert np.allclose(sample.rgb.shape[1:], image_size)\n",
    "\n",
    "    process_data = e2e_processor(sample)\n",
    "\n",
    "    fig = vis.overview_fig(\n",
    "        fig=plt.figure(figsize=(20, 20)),\n",
    "        original_rgb=vis.make_tensor_displayable(process_data[\"sample\"].rgb, True, True),\n",
    "        preprocessed_rgb=vis.make_tensor_displayable(\n",
    "            process_data[\"preprocessor\"][\"rgb_masked\"], True, True\n",
    "        ),\n",
    "        q_img=vis.make_tensor_displayable(\n",
    "            process_data[\"postprocessor\"][\"q_img\"], False, False\n",
    "        ),\n",
    "        angle_img=vis.make_tensor_displayable(\n",
    "            process_data[\"postprocessor\"][\"angle_img\"], False, False\n",
    "        ),\n",
    "        width_img=vis.make_tensor_displayable(\n",
    "            process_data[\"postprocessor\"][\"width_img\"], False, False\n",
    "        ),\n",
    "        image_grasps=process_data[\"grasps_img\"],\n",
    "        world_grasps=process_data[\"grasps_world\"],\n",
    "        cam_intrinsics=cam_intrinsics,\n",
    "        cam_rot=cam_rot,\n",
    "        cam_pos=cam_pos,\n",
    "    )\n",
    "\n",
    "    export_data = {\n",
    "        \"rgb_cropped\": process_data[\"preprocessor\"][\"rgb_cropped\"],\n",
    "        \"depth_cropped\": process_data[\"preprocessor\"][\"depth_cropped\"],\n",
    "        \"rgb_masked\": process_data[\"preprocessor\"][\"rgb_masked\"],\n",
    "        \"q_img\": process_data[\"postprocessor\"][\"q_img\"],\n",
    "        \"angle_img\": process_data[\"postprocessor\"][\"angle_img\"],\n",
    "        \"width_img\": process_data[\"postprocessor\"][\"width_img\"],\n",
    "        \"grasps_img\": process_data[\"grasps_img\"],\n",
    "        \"grasps_world\": process_data[\"grasps_world\"],\n",
    "        \"model_input\": process_data[\"model_input\"],\n",
    "        \"overview\": fig,\n",
    "    }\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "    export_path = exporter(export_data, f\"{process_data['sample'].name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('alr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ef7e1541b8e7b6b5672ac838d3c045be09c9245709d40ce12336bbbdd1b51144"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
