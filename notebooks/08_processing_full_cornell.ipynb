{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import yaml\n",
    "\n",
    "from grconvnet.dataloading.datasets import CornellDataset\n",
    "from grconvnet.utils.processing import End2EndProcessor\n",
    "from grconvnet.postprocessing import Img2WorldCoordConverter, Decropper\n",
    "from grconvnet.utils.config import module_from_config\n",
    "from grconvnet.utils import visualization as vis\n",
    "from grconvnet.utils.export import Exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(\"/home/moritz/Documents/cornell\")\n",
    "config_path = Path.cwd().parent / \"grconvnet\" / \"configs\" / \"cornell_inference_standard.yaml\"\n",
    "export_path = Path.cwd().parent / \"grconvnet\" / \"results\" / \"cornell_standard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pcd0100'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = CornellDataset(dataset_path)\n",
    "sample = dataset[0]\n",
    "sample.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2e_processor = module_from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "exporter = Exporter(export_dir=export_path)\n",
    "\n",
    "export_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(export_path / \"inference_config.yaml\", \"w\") as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "with open(export_path / \"dataset_config.yaml\", \"w\") as f:\n",
    "    yaml.dump(dataset_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample pcd0100...\n",
      "Processing sample pcd0101...\n",
      "Processing sample pcd0102...\n",
      "Processing sample pcd0103...\n",
      "Processing sample pcd0104...\n",
      "Processing sample pcd0105...\n",
      "Processing sample pcd0106...\n",
      "Processing sample pcd0107...\n",
      "Processing sample pcd0108...\n",
      "Processing sample pcd0109...\n",
      "Processing sample pcd0110...\n",
      "Processing sample pcd0111...\n",
      "Processing sample pcd0112...\n",
      "Processing sample pcd0113...\n",
      "Processing sample pcd0114...\n",
      "Processing sample pcd0115...\n",
      "Processing sample pcd0116...\n",
      "Processing sample pcd0117...\n",
      "Processing sample pcd0118...\n",
      "Processing sample pcd0119...\n",
      "Processing sample pcd0120...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10331/1239016748.py:7: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig=plt.figure(figsize=(20, 20)),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample pcd0121...\n",
      "Processing sample pcd0122...\n",
      "Processing sample pcd0123...\n",
      "Processing sample pcd0124...\n",
      "Processing sample pcd0125...\n",
      "Processing sample pcd0126...\n",
      "Processing sample pcd0127...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m dataset:\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProcessing sample \u001b[39m\u001b[39m{\u001b[39;00msample\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     process_data \u001b[39m=\u001b[39m e2e_processor(sample)\n\u001b[1;32m      6\u001b[0m     fig \u001b[39m=\u001b[39m vis\u001b[39m.\u001b[39moverview_fig(\n\u001b[1;32m      7\u001b[0m         fig\u001b[39m=\u001b[39mplt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m20\u001b[39m, \u001b[39m20\u001b[39m)),\n\u001b[1;32m      8\u001b[0m         original_rgb\u001b[39m=\u001b[39mvis\u001b[39m.\u001b[39mmake_tensor_displayable(process_data[\u001b[39m\"\u001b[39m\u001b[39msample\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mrgb, \u001b[39mTrue\u001b[39;00m, \u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m         image_grasps\u001b[39m=\u001b[39mprocess_data[\u001b[39m\"\u001b[39m\u001b[39mgrasps_img\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     24\u001b[0m     export_data \u001b[39m=\u001b[39m {\n\u001b[1;32m     25\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrgb_cropped\u001b[39m\u001b[39m\"\u001b[39m: process_data[\u001b[39m\"\u001b[39m\u001b[39mpreprocessor\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mrgb_cropped\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     26\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdepth_cropped\u001b[39m\u001b[39m\"\u001b[39m: process_data[\u001b[39m\"\u001b[39m\u001b[39mpreprocessor\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mdepth_cropped\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39moverview\u001b[39m\u001b[39m\"\u001b[39m: fig,\n\u001b[1;32m     34\u001b[0m     }\n",
      "File \u001b[0;32m~/Documents/GrConvNetBenchmark/grconvnet/utils/processing.py:43\u001b[0m, in \u001b[0;36mEnd2EndProcessor.__call__\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m     40\u001b[0m input_tensor \u001b[39m=\u001b[39m input_tensor\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 43\u001b[0m     prediction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(input_tensor)\n\u001b[1;32m     45\u001b[0m grasps_img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocessor(prediction)\n\u001b[1;32m     47\u001b[0m \u001b[39m# TODO save all intermediate results of the img2world converter\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/alr/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GrConvNetBenchmark/grconvnet/models/grconvnet.py:133\u001b[0m, in \u001b[0;36mGenerativeResnet.forward\u001b[0;34m(self, x_in)\u001b[0m\n\u001b[1;32m    131\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn4(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv4(x)))\n\u001b[1;32m    132\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn5(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv5(x)))\n\u001b[0;32m--> 133\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv6(x)\n\u001b[1;32m    135\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout:\n\u001b[1;32m    136\u001b[0m     pos_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_output(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout_pos(x))\n",
      "File \u001b[0;32m~/mambaforge/envs/alr/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/alr/lib/python3.10/site-packages/torch/nn/modules/conv.py:956\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    951\u001b[0m num_spatial_dims \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    952\u001b[0m output_padding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_padding(\n\u001b[1;32m    953\u001b[0m     \u001b[39minput\u001b[39m, output_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_size,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    954\u001b[0m     num_spatial_dims, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 956\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv_transpose2d(\n\u001b[1;32m    957\u001b[0m     \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[1;32m    958\u001b[0m     output_padding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for sample in dataset:\n",
    "    print(f\"Processing sample {sample.name}...\")\n",
    "\n",
    "    process_data = e2e_processor(sample)\n",
    "\n",
    "    fig = vis.overview_fig(\n",
    "        fig=plt.figure(figsize=(20, 20)),\n",
    "        original_rgb=vis.make_tensor_displayable(process_data[\"sample\"].rgb, True, True),\n",
    "        preprocessed_rgb=vis.make_tensor_displayable(\n",
    "            process_data[\"preprocessor\"][\"rgb_masked\"], True, True\n",
    "        ),\n",
    "        q_img=vis.make_tensor_displayable(\n",
    "            process_data[\"postprocessor\"][\"q_img\"], False, False\n",
    "        ),\n",
    "        angle_img=vis.make_tensor_displayable(\n",
    "            process_data[\"postprocessor\"][\"angle_img\"], False, False\n",
    "        ),\n",
    "        width_img=vis.make_tensor_displayable(\n",
    "            process_data[\"postprocessor\"][\"width_img\"], False, False\n",
    "        ),\n",
    "        image_grasps=process_data[\"grasps_img\"],\n",
    "    )\n",
    "\n",
    "    export_data = {\n",
    "        \"rgb_cropped\": process_data[\"preprocessor\"][\"rgb_cropped\"],\n",
    "        \"depth_cropped\": process_data[\"preprocessor\"][\"depth_cropped\"],\n",
    "        \"rgb_masked\": process_data[\"preprocessor\"][\"rgb_masked\"],\n",
    "        \"q_img\": process_data[\"postprocessor\"][\"q_img\"],\n",
    "        \"angle_img\": process_data[\"postprocessor\"][\"angle_img\"],\n",
    "        \"width_img\": process_data[\"postprocessor\"][\"width_img\"],\n",
    "        \"grasps_img\": process_data[\"grasps_img\"],\n",
    "        \"model_input\": process_data[\"model_input\"],\n",
    "        \"overview\": fig,\n",
    "    }\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "    export_path = exporter(export_data, f\"{process_data['sample'].name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('alr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ef7e1541b8e7b6b5672ac838d3c045be09c9245709d40ce12336bbbdd1b51144"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
